{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Estate Valuation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "from numpy import mean,std\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import cross_val_score,RepeatedKFold,RandomizedSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "from sklearn.ensemble import BaggingRegressor,RandomForestRegressor,VotingRegressor,IsolationForest\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Importing dataset\n",
    "# Importing or Loading dataset\n",
    "data = \"C:/Users/Omomule Taiwo G/Desktop/Dataset/Regression/Real_Estate/real_estate.csv\"\n",
    "df = pd.read_csv(data, delimiter=',')\n",
    "# Inspect data\n",
    "print('Inspect Data')\n",
    "#print(df.to_string())\n",
    "# Check Shape\n",
    "print(df.shape)\n",
    "# Statistical Summary\n",
    "print(df.describe())\n",
    "\n",
    "# Data Types\n",
    "print(df.info())\n",
    "\n",
    "\n",
    "# Each feature summary\n",
    "for i in df:\n",
    "    print(df[i].describe())\n",
    "# Inspecting the statistical summary of the data shows the need for rescaling.\n",
    "\n",
    "# Check Missing Values: To delete columns having missing values more than 30% or to input values--------\n",
    "# Check missing values\n",
    "df3 = df.isnull().sum()\n",
    "print('Missing values in each feature \\n:-------------------------------')\n",
    "print(df3) # There are no missing values in the data\n",
    "\n",
    "\n",
    "# Check feature relevance to the target through correlation matrix\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(df.corr(), annot=True)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "df_corr = df.corr()\n",
    "print('Feature Correlation Table')\n",
    "print(df_corr)\n",
    "\n",
    "# Through inspection, the real estate dataset is a continuous dataset, having float feature values\n",
    "# and no missing values. The data is well scaled. Also, the target attributes contains floating values.\n",
    "\n",
    "# Feature Engineering: Select Relevant features by evaluating feature importance (Dimensionality Reduction)\n",
    "#  By performing feature correlation and viewing the data, features like 'No' needs to be dropped because it is just\n",
    "# serial no.\n",
    "# Drop No and transaction data from the data\n",
    "df = df.drop(['No'], axis=1)\n",
    "#print(df.to_string())\n",
    "#print('New Data Shape',df.shape)\n",
    "\n",
    "\n",
    "# Separate feature vectors from target labels\n",
    "X = df.drop('Y house price of unit area',axis=1)\n",
    "#print(X.to_string())\n",
    "y = df['Y house price of unit area'].copy()\n",
    "#print(y.to_string())\n",
    "\n",
    "\n",
    "# Visualize the data using Histogram plots\n",
    "# plot the histograms of all features or variable in the data\n",
    "X.hist(sharex=False, sharey=False,  xlabelsize=1, ylabelsize=1, figsize=(4,4))\n",
    "plt.show()\n",
    "# Note: In the plots, The shape of the each graph can be Gaussianâ€™, skewed or even has an exponential distribution.\n",
    "# Density plots\n",
    "X.plot(kind='density', subplots=True, layout=(5,5), sharex=False, legend=True, fontsize=1, figsize=(4,4))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Convert the Dataframe to Numpy Arrays\n",
    "X = X.values\n",
    "y = y.values\n",
    "\n",
    "# Check Outliers-------------------------------------------\n",
    "# identify outliers using Isolation Forest in the training dataset\n",
    "# The data has been transformed between a range of 0 and 1. So there is no need to check for outliers\n",
    "# identify outliers using Isolation Forest in the training dataset\n",
    "iso = IsolationForest(contamination=0.1)\n",
    "# Contamination argument is used to help estimate the number of outliers in the dataset.\n",
    "# This is a value between 0.0 and 0.5 and by default is set to 0.1.\n",
    "outl = iso.fit_predict(X)\n",
    "# select all rows that are not outliers\n",
    "remove_outl = outl != -1\n",
    "X, y = X[remove_outl, :], y[remove_outl]\n",
    "# summarize the shape of the updated training dataset\n",
    "print('New data without outliers \\n')\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "\n",
    "# Performing feature normalization or standardization-----------------------------------\n",
    "scale = MinMaxScaler()# The MinMax Scaler normalizes the data to a range 0 and 1.\n",
    "X = scale.fit_transform(X)\n",
    "#print(X)\n",
    "\n",
    "# Performing feature normalization or standardization-----------------------------------\n",
    "scale = MinMaxScaler()# The MinMax Scaler normalizes the data to a range 0 and 1.\n",
    "X = scale.fit_transform(X)\n",
    "#print(X)\n",
    "\n",
    "\n",
    "# DATA PREPARATION ENDS HERE---------------------------------------------------------------------\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "print('\\n')\n",
    "# MODEL DEVELOPMENT BEGINS\n",
    "print('# MODEL DEVELOPMENT BEGINS')\n",
    "# Cross validation of 10 folds and 5 runs\n",
    "cv_method = RepeatedKFold(n_splits=10, n_repeats=5, random_state=42)\n",
    "\n",
    "# Hyperparameter Optimization using RandomSearch and CrossValidation to get the best model hyperparamters\n",
    "\n",
    "# naive Bayes Classifier\n",
    "\n",
    "# kNN Classifier\n",
    "nearest_neighbour = KNeighborsRegressor()\n",
    "# Create a dictionary of KNN parameters\n",
    "# K values between 1 and 9 are used to avoid ties and p values of 1 (Manhattan), 2 (Euclidean), and 5 (Minkowski)\n",
    "param_kNN = {'n_neighbors': [1,3,5,7,9],'p':[1,2,5]} # Distance Metric: Manhattan (p=1), Euclidean (p=2) or\n",
    "# Minkowski (any p larger than 2). Technically p=1 and p=2 are also Minkowski distances.\n",
    "# Define the kNN model using RandomSearch and optimize accuracy\n",
    "kNN_grid = RandomizedSearchCV(nearest_neighbour,param_kNN,scoring='r2',cv=cv_method)\n",
    "kNN_grid.fit(X_train,y_train)\n",
    "# Print the best parameter values for KNN\n",
    "print('kNN Best Parameter values =',kNN_grid.best_params_)\n",
    "#kNN = KNeighborsRegressor(**kNN_grid.best_params_)\n",
    "kNN = kNN_grid.best_estimator_\n",
    "\n",
    "\n",
    "# Decision Tree Classifier\n",
    "Decision_Tree = DecisionTreeRegressor()\n",
    "# Create a dictionary of DT hyperparameters\n",
    "params_DT = {'criterion':['mse','mae'],\n",
    "             'max_depth':[1,2,3,4,5,6,7,8],\n",
    "             'splitter':['best','random']}\n",
    "\n",
    "# Using Random Search to explore the best parameter for the a decision tree model\n",
    "DT_Grid = RandomizedSearchCV(Decision_Tree,params_DT,scoring='r2',cv=cv_method)\n",
    "# Fitting the parameterized model\n",
    "DT_Grid.fit(X_train,y_train)\n",
    "# Print the best parameter values\n",
    "print('DT Best Parameter Values:', DT_Grid.best_params_)\n",
    "#DT = DecisionTreeRegressor(**DT_Grid.best_params_)\n",
    "DT = DT_Grid.best_estimator_\n",
    "\n",
    "\n",
    "# Support Vector Machines\n",
    "SVM_clasf = SVR()\n",
    "# Create a dictionary of SVM hyperparameters\n",
    "# Parameter space for rbf kernels\n",
    "params_SVR = {'kernel':['rbf'],'C':np.linspace(0.1,1.0),\n",
    "              'gamma':['scale','auto']} #np.linspace(0.1,1.0)}\n",
    "\n",
    "# Using Random Search to explore the best parameter for the a SVM model\n",
    "SVR_Grid = RandomizedSearchCV(SVM_clasf,params_SVR,scoring='r2',cv=cv_method)\n",
    "# Fitting the parameterized model\n",
    "SVR_Grid.fit(X_train,y_train)\n",
    "# Print the best parameter values\n",
    "print('SVR Best Parameter Values:', SVR_Grid.best_params_)\n",
    "#SVR = SVR(**SVR_Grid.best_params_)\n",
    "SVR = SVR_Grid.best_estimator_\n",
    "\n",
    "\n",
    "# Neural Network\n",
    "mlp = MLPRegressor()\n",
    "parameter_MLP = {\n",
    "    'hidden_layer_sizes': [(25,25,25),(50,50,50),(100,100,100)],\n",
    "    'activation': ['relu','tanh'],\n",
    "    'solver': ['adam'],'max_iter':[500,1000],\n",
    "    'learning_rate': ['constant','adaptive']}\n",
    "\n",
    "mlp_Grid = RandomizedSearchCV(mlp, parameter_MLP, scoring='r2',cv=cv_method)\n",
    "mlp_Grid.fit(X_train, y_train) # X is train samples and y is the corresponding labels\n",
    "\n",
    "# Check best hyperparameters\n",
    "print('ANN Best parameter values:\\n', mlp_Grid.best_params_)\n",
    "#MLP = MLPRegressor(**mlp_Grid.best_params_)\n",
    "MLP = mlp_Grid.best_estimator_\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "# Developing homogeneous ensembles of each classifier\n",
    "kNN_ensemble = BaggingRegressor(base_estimator=kNN,n_estimators=10)\n",
    "DT_ensemble = BaggingRegressor(base_estimator=DT,n_estimators=10)\n",
    "Rand_forest = RandomForestRegressor(n_estimators=10)\n",
    "SVM_ensemble = BaggingRegressor(base_estimator=SVR,n_estimators=10)\n",
    "MLP_ensemble = BaggingRegressor(base_estimator=MLP,n_estimators=10)\n",
    "\n",
    "\n",
    "def get_HTRGN_ensemble():\n",
    "    models = list()\n",
    "    models.append(('kNN_ensemble', kNN_ensemble))\n",
    "    models.append(('DT_ensemble', DT_ensemble))\n",
    "    models.append(('RF', Rand_forest))\n",
    "    models.append(('SVM_ensemble', SVM_ensemble))\n",
    "    models.append(('MLP_ensemble', MLP_ensemble))\n",
    "    HTE = VotingRegressor(estimators=models)\n",
    "    return HTE\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    models['kNNR_HE'] = kNN_ensemble\n",
    "    models['DTR_HE'] = DT_ensemble\n",
    "    models['RF'] = Rand_forest\n",
    "    models['SVR_HE'] = SVM_ensemble\n",
    "    models['ANNR_HE'] = MLP_ensemble\n",
    "    models['HTE'] = get_HTRGN_ensemble()\n",
    "    return models\n",
    "\n",
    "\n",
    "# Cross validate the models\n",
    "def evaluate_model(model, X_train, y_train):\n",
    "    # evaluate the model and collect the results\n",
    "    scores = cross_val_score(model, X_train, y_train, scoring='r2', cv=cv_method, n_jobs=-1)\n",
    "    return scores\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "print('Cross Validation R-squared values of each ensemble on test set:------------------------------------------------------')\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X_test, y_test)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print('>%s %.3f' % (name, mean(scores)), u\"\\u00B1\", '%.3f' % std(scores))\n",
    "\n",
    "# plot model performance for comparison\n",
    "plt.boxplot(results, labels=names, showfliers=False)\n",
    "plt.title('Cross validation R-squared comparison of ensembles')\n",
    "plt.show()\n",
    "print('\\n')\n",
    "\n",
    "print('Cross validation R-squared values of ensemble on train set:----------------------------------------------------')\n",
    "for name, model in models.items():\n",
    "    # evaluate the model\n",
    "    scores = evaluate_model(model, X_train, y_train)\n",
    "    # store the results\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    # summarize the performance along the way\n",
    "    print('>%s %.3f' % (name, mean(scores)), u\"\\u00B1\", '%.3f' % std(scores))\n",
    "\n",
    "print('\\n')\n",
    "# Train and evaluate each Ensemble\n",
    "for name,model in models.items():\n",
    "    # fit the model\n",
    "    model.fit(X_train,y_train)\n",
    "    # then predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Evaluate the models\n",
    "    #y_pred = enc.inverse_transform(y_pred)\n",
    "    mse_test = mean_squared_error(y_test, y_pred)\n",
    "    root_mse_test = np.sqrt(mse_test)\n",
    "    r2_test= r2_score(y_test,y_pred)\n",
    "    print('Performance Result of',name,':-----------------------------------------------------------------')\n",
    "    print(name, 'root mean squared error of test set:', root_mse_test)\n",
    "    print(name, 'r_squared coefficient of test set:', r2_test)\n",
    "    y_pred1 = model.predict(X_train)\n",
    "    mse_train = mean_squared_error(y_train,y_pred1)\n",
    "    root_mse_train = np.sqrt(mse_train)\n",
    "    r2_train = r2_score(y_train, y_pred1)\n",
    "    print(name, 'root mean squared error of train set:', root_mse_train)\n",
    "    print(name, 'r_squared coefficient of train set:',r2_train)\n",
    "    gen_factor = mse_test / mse_train\n",
    "    print('Generalization Factor to determine Ensemble Overfitting', gen_factor)\n",
    "    # NOTE: if the gen_factor > 1, then the ensemble overfits else it is desirable\n",
    "    print('\\n')\n",
    "\n",
    "    # Evaluate Bias-Variance Tradeoff\n",
    "    avg_expected_loss2, avg_bias2, avg_variance2 = bias_variance_decomp(model, X_train, y_train\n",
    "                                                                        , X_test, y_test, loss='mse',\n",
    "                                                                        num_rounds=10,\n",
    "                                                                        random_seed=20)\n",
    "    # Summary of Results\n",
    "    print('Average Expected loss for', name, '%.2f' % avg_expected_loss2)\n",
    "    print('Average Expected Bias error for', name, '%.2f' % avg_bias2)\n",
    "    print('Average Expected Variance error for', name, '%.2f' % avg_variance2)\n",
    "    print('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
